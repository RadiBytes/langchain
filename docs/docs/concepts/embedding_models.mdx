# Embedding models
<span data-heading-keywords="embedding,embeddings"></span>

## Overview

Imagine being able to capture the essence of any text - a tweet, a document, or even an entire book - in a single, compact representation. 
This is the power of embedding models, which lie at the heart of many retrieval systems.
Embedding models transform human language into a format that machines can understand and compare with speed and accuracy. 
These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning:

- **Semantic Capture**: Each embedding is essentially a set of coordinates in a vast, abstract space. In this space, the position of each point (embedding) reflects the meaning of its corresponding text.
- **Similarity as Proximity**: Just as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space. This allows for intuitive comparisons between different pieces of text.
- **Efficient Comparison**: By reducing text to these numerical representations, we can use simple mathematical operations like cosine similarity to quickly measure how alike two pieces of text are, regardless of their original length or structure.

Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding. 

## Key concepts

![Conceptual Overview](/img/embeddings_concept.png)

 (1) **Embed text as a vector**: Embedding models transform text into a fixed-length array of floating point numbers.

 (2) **Measure the similarity between vectors**: Embeddings can then be compared using simple mathematical operations.

## Embedding  

The landscape of embedding models has evolved significantly over the years. 
A pivotal moment came in 2018 when Google introduced [BERT (Bidirectional Encoder Representations from Transformers)](https://www.nvidia.com/en-us/glossary/bert/). 
BERT applied transformer models to embed text as a simple vector representation, which lead to unprecedented performance across various NLP tasks.
However, BERT wasn't optimized for generating sentence embeddings efficiently. 
This limitation spurred the creation of [SBERT (Sentence-BERT)](https://www.sbert.net/examples/training/sts/README.html), which adapted the BERT architecture to generate semantically rich sentence embeddings, easily comparable via similarity metrics like cosine similarity, dramatically reduced the computational overhead for tasks like finding similar sentences.

Today, the embedding model ecosystem is diverse, with numerous providers offering their own implementations. 
To navigate this variety, researchers and practitioners often turn to benchmarks like the Massive Text Embedding Benchmark (MTEB) [here](https://huggingface.co/blog/mteb) for objective comparisons.
Recognizing the need for a unified approach to working with these diverse models, LangChain introduced the `Embeddings` class.
 This common interface simplifies interaction with various embedding providers through two key methods:

- `embed_documents`: For embedding multiple texts (documents)
- `embed_query`: For embedding a single text (query)

This distinction is crucial, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself).
To illustrate, here's a practical example using LangChain's `.embed_documents` method to embed a list of strings:

```python
from langchain_openai import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings()
embeddings = embeddings_model.embed_documents(
    [
        "Hi there!",
        "Oh, hello!",
        "What's your name?",
        "My friends call me World",
        "Hello World!"
    ]
)
len(embeddings), len(embeddings[0])
(5, 1536)
```

For convenience, you can also use the `embed_query` method to embed a single text:

```python
query_embedding = embeddings_model.embed_query("What is the meaning of life?")
```

For specifics on how to use embedding models, see the [relevant how-to guides here](/docs/how_to/embed_text).

### Embedding with higher granularity  

![](/img/embeddings_colbert.png)

Embedding models compress text into fixed-length (vector) representations, which can put a heavy burden on that single vector to capture the semantic nuance and detail of the document. 
In some cases, irrelevant or redundant content can dilute the semantic usefulness of the embedding.
[ColBERT](https://arxiv.org/abs/2004.12832) (Contextualized Late Interaction over BERT) is an innovative approach to address this limitation by using higher granularity embeddings. 
Here's how ColBERT works:

- **Token-level embeddings**: Produce contextually influenced embeddings for each token in the document and the query.
- **MaxSim operation**: For each query token, compute its maximum similarity with all document tokens.
- **Aggregation**: The final relevance score is obtained by summing these maximum similarities across all query tokens.

This token-wise scoring can yield strong results, especially for tasks requiring precise matching or handling longer documents.
Key advantages of ColBERT:

- **Improved accuracy**: Token-level interactions can capture more nuanced relationships between query and document.
- **Interpretability**: The token-level matching allows for easier interpretation of why a document was considered relevant.

However, ColBERT does come with some trade-offs:

- **Increased computational cost**: Processing and storing token-level embeddings requires more resources.
- **Complexity**: Implementing and optimizing ColBERT can be more challenging than simpler embedding models.

| Name              | When to use                                              | Description                                                                                                                                                                            |
|-------------------|----------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [ColBERT](/docs/integrations/providers/ragatouille/#using-colbert-as-a-reranker)           | When higher granularity embeddings are needed.           | ColBERT uses contextually influenced embeddings for each token in the document and query to get a granular query-document similarity score. [Paper](https://arxiv.org/abs/2112.01488). |

:::tip

See our RAG from Scratch video on [ColBERT](https://youtu.be/cN6S0Ehm7_8?feature=shared>).

:::

## Similarity 

With text compressed into a vector representation, the next question is how to measure the similarity between text chunks. 
Fortunately, we can use simple mathematical operations to measure the similarity between vectors. 
For example, OpenAI recommends [cosine similarity](https://platform.openai.com/docs/guides/embeddings/faq) as a similarity metric. 
OpenAI embeddings are normalized to length 1, which means that Cosine similarity can be computed slightly faster using just a dot product
and Cosine similarity and Euclidean distance will result in the identical rankings. Any two embedded texts can be compared using this function:

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    return dot_product / (norm_vec1 * norm_vec2)

similarity = cosine_similarity(query_result, document_result)
print("Cosine Similarity:", similarity)
```  

However, the choice of similarity metric should be informed by the specific use case and embedding model.
The central concept is the once embedding model is used to convert texts into vectors, it is fairly straightforward to compute the similarity between texts. 